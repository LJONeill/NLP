{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0b174f1",
   "metadata": {},
   "source": [
    "This notebook is an attempt at re-creating how we created our train and test data, as we lost our original notebook due to a technical issue, and by the time we realized it was not recoverable.\n",
    "Our goal with our splits was that the test data was overlapping, so if we ran the smallest test set, the test_9010 set, on any of the models, none of them would have \"seen\" it before - hence, each smaller test-set should be a balanced sub-set of the largest test-set, ensuring no overlap between training and test data.\n",
    "\n",
    "Therefore, re-running this notebook will not create identical test and training data to ours, however, all the data we've used is stored in the /data subfolders (paths are as below) in this repository and can be used to re-create our models and test results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9253406",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05491a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned IMDB dataset\n",
    "df = pd.read_csv('../data/imdb_datasets/clean_imdb_dataset.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ce4eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Split dataset into 90% train and 10% test\n",
    "X = df['review']\n",
    "y = df['sentiment']\n",
    "\n",
    "X_train_90, X_test_10, y_train_90, y_test_10 = train_test_split(\n",
    "    X, y, test_size=0.1, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "train_90 = pd.DataFrame({'review': X_train_90, 'sentiment': y_train_90})\n",
    "test_90 = pd.DataFrame({'review': X_test_10, 'sentiment': y_test_10})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8b0977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create train_50 and train_10 subsets from the 90% training data\n",
    "X_train_50, _, y_train_50, _ = train_test_split(\n",
    "    X_train_90, y_train_90, train_size=50/90, stratify=y_train_90, random_state=42\n",
    ")\n",
    "X_train_10, _, y_train_10, _ = train_test_split(\n",
    "    X_train_90, y_train_90, train_size=10/90, stratify=y_train_90, random_state=42\n",
    ")\n",
    "\n",
    "train_50 = pd.DataFrame({'review': X_train_50, 'sentiment': y_train_50})\n",
    "train_10 = pd.DataFrame({'review': X_train_10, 'sentiment': y_train_10})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6175d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Save all splits to CSV\n",
    "#train_90.to_csv('../data/imdb_test_train_datasets/train/train_9010.csv', index=False)\n",
    "#train_50.to_csv('../data/imdb_test_train_datasets/train/train_5050.csv', index=False)\n",
    "#train_10.to_csv('../data/imdb_test_train_datasets/train/train_1090.csv', index=False)\n",
    "\n",
    "#test_90.to_csv('../data/imdb_test_train_datasets/test/test_9010.csv', index=False)\n",
    "#test_50.to_csv('../data/imdb_test_train_datasets/test/test_5050.csv', index=False)\n",
    "#test_10.to_csv('../data/imdb_test_train_datasets/test/test_1090.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv (Python 3.11)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
