{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cb7fa69",
   "metadata": {},
   "source": [
    "# DISCLAIMER!!\n",
    "\n",
    "This notebook serves as a reference for the .py files we used to fine-tune and evaluate our sentiment analysis model on the test data. Due to limited computational resources, we were unable to execute this notebook directly. Instead, we generated predictions and trained the model using standalone .py scripts with HPC based on the code and structure outlined here. Meaning, this notebook is the skeleton/initial reference for what became our .py-scripts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220a32a1",
   "metadata": {},
   "source": [
    "# Fine-tuning BERT on IMDB Sentiment Data\n",
    "\n",
    "This notebook demonstrates how to fine-tune a pretrained BERT model for sentiment classification using an IMDB dataset. It includes steps for loading the dataset, preprocessing the text, training the model, evaluating it, and generating predictions along with confidence scores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6114785d",
   "metadata": {},
   "source": [
    "## üîç What the Notebook Does\n",
    "\n",
    "This notebook performs the **full fine-tuning pipeline** on the IMDB dataset:\n",
    "\n",
    "-  Loads the **pre-cleaned IMDB reviews and sentiment labels**.\n",
    "-  Encodes the sentiment labels (`positive` ‚Üí 1, `negative` ‚Üí 0).\n",
    "-  Splits the dataset into **training and validation sets**.\n",
    "-  Loads the **BERT tokenizer and base model** (`bert-base-uncased`).\n",
    "-  Wraps the text and labels in a `torch.utils.data.Dataset` object.\n",
    "-  Defines **training hyperparameters** and **evaluation strategy**.\n",
    "-  **Fine-tunes** the model on your sentiment classification task using Hugging Face's `Trainer` API.\n",
    "-  Saves the **fine-tuned model and tokenizer** locally.\n",
    "-  Reloads the fine-tuned model and runs **inference on the full dataset**, generating:\n",
    "  - **Predicted sentiment** (`positive` / `negative`)\n",
    "  - **Confidence score** (from softmax probabilities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f31149d",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup\n",
    "\n",
    "We begin by importing necessary libraries for data handling, model loading, training, and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb21e4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05234ad",
   "metadata": {},
   "source": [
    "## 2. Load and Encode Dataset\n",
    "\n",
    "Load the cleaned IMDB dataset and encode sentiment labels to numerical values using `LabelEncoder`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74dd591",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../data/IMDB_datasets/clean_imdb_dataset.csv\")\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"label\"] = label_encoder.fit_transform(df[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ea1f32",
   "metadata": {},
   "source": [
    "## 3. Train-Test Split\n",
    "\n",
    "Split the dataset into training and validation sets (90% train, 10% validation).\n",
    "\n",
    "(not sure if this should be the split for the fine tuning, but i gues using 90% makes sense?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731788f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df[\"review\"].tolist(), df[\"label\"].tolist(), test_size=0.9, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1724497a",
   "metadata": {},
   "source": [
    "## 4. Load Tokenizer\n",
    "\n",
    "We load the `bert-base-uncased` tokenizer from Hugging Face, which converts text into tokens the model understands.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b8844f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e828a84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=512):#need to double check\n",
    "        self.encodings = tokenizer(texts, padding=True, truncation=True, max_length=max_len, return_tensors=\"pt\")\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: v[idx] for k, v in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425efc54",
   "metadata": {},
   "source": [
    "## 5. Create Dataset Wrapper\n",
    "\n",
    "Define a custom dataset class to prepare tokenized inputs and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3032df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=512):\n",
    "        self.encodings = tokenizer(texts, padding=True, truncation=True, max_length=max_len, return_tensors=\"pt\")\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: v[idx] for k, v in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb8b7c0",
   "metadata": {},
   "source": [
    "## 6. Prepare Dataset Objects\n",
    "\n",
    "Convert training and validation data into instances of `IMDBDataset`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e57bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = IMDBDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = IMDBDataset(val_texts, val_labels, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4269adb5",
   "metadata": {},
   "source": [
    "## 7. Load Pretrained BERT Model\n",
    "\n",
    "Load the pretrained BERT model with a classification head (2 output labels: positive and negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0377e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a8a995",
   "metadata": {},
   "source": [
    "## 8. Training Arguments\n",
    "\n",
    "Define how the model should be trained, including learning rate, batch size, number of epochs, and evaluation strategy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99356e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    save_total_limit=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3aaa657",
   "metadata": {},
   "source": [
    "## 9. Define Evaluation Metric\n",
    "\n",
    "Use the `evaluate` library to calculate accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca7a79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "import numpy as np\n",
    "\n",
    "accuracy = load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    return accuracy.compute(predictions=preds, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074fdec0",
   "metadata": {},
   "source": [
    "## 10. Train the Model\n",
    "\n",
    "Initialize the `Trainer` class and train the model using the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460173d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3381bf68",
   "metadata": {},
   "source": [
    "## 11. Save Fine-tuned Model\n",
    "\n",
    "Save the trained model and tokenizer locally for later inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3dafd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./fine_tuned_bert_imdb\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_bert_imdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53794682",
   "metadata": {},
   "source": [
    "## 12. Use Fine-tuned Model for Predictions on Full Dataset\n",
    "\n",
    "Reload the model and tokenizer, and apply them to the entire dataset to predict sentiment labels and confidence scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a192c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained(\"./fine_tuned_bert_imdb\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./fine_tuned_bert_imdb\")\n",
    "model.eval()\n",
    "\n",
    "# Encode full dataset\n",
    "texts = df[\"review\"].tolist()\n",
    "encodings = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encodings)\n",
    "    probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "    predictions = torch.argmax(probs, dim=1)\n",
    "    confidences = torch.max(probs, dim=1).values.tolist()\n",
    "\n",
    "# Decode predictions\n",
    "predicted_labels = label_encoder.inverse_transform(predictions.tolist())\n",
    "\n",
    "# Save results to DataFrame\n",
    "df[\"predicted_sentiment\"] = predicted_labels\n",
    "df[\"confidence\"] = confidences\n",
    "\n",
    "# Export results\n",
    "df.to_csv(\"../data/imdb_predictions_with_confidence.csv\", index=False)\n",
    "print(\"Saved predictions to ../data/imdb_predictions_with_confidence.csv\")\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
