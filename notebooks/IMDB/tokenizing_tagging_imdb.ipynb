{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dcebe88",
   "metadata": {},
   "source": [
    "# Tagging Cleaned IMDB Dataset using Baseline NER Model\n",
    "This notebook loads your trained NER model from a checkpoint and uses it to tag entities in the cleaned IMDB dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79d2452",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../scripts\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8f7fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModelForTokenClassification, RobertaTokenizerFast, AutoConfig, Trainer, TrainingArguments, DataCollatorForTokenClassification\n",
    "from datasets import Dataset\n",
    "from span_f1 import readNlu, toSpans, getBegEnd, getLooseOverlap, getUnlabeled\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea25447",
   "metadata": {},
   "source": [
    "## 1. Load Cleaned IMDB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ea3434",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/clean_imdb_dataset.csv\")\n",
    "df['tokens'] = df['review'].apply(lambda x: x.split())\n",
    "df['dummy_labels'] = df['tokens'].apply(lambda x: ['O'] * len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe88f5fb",
   "metadata": {},
   "source": [
    "## 2. Create HuggingFace Dataset Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1d2038",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_data = Dataset.from_dict({\n",
    "    'sents': df['tokens'].tolist(),\n",
    "    'ner_tags': df['dummy_labels'].tolist(),\n",
    "    'ids': df['dummy_labels'].tolist()\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d38b2fa",
   "metadata": {},
   "source": [
    "## 3. Load Tokenizer and Label Mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d6bf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load label mappings used during training\n",
    "with open('../project/baseline_model/idx2lab', 'rb') as f:\n",
    "    idx2lab = pickle.load(f)\n",
    "\n",
    "with open('../project/baseline_model/lab2idx', 'rb') as f:\n",
    "    lab2idx = pickle.load(f)\n",
    "\n",
    "label_list = list(lab2idx.keys())  # Needed for num_labels\n",
    "\n",
    "# Tokenizer and config\n",
    "model_link = \"deepset/roberta-base-squad2\"\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(model_link, use_fast=True, add_prefix_space=True)\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_link,\n",
    "    num_labels=len(label_list),\n",
    "    id2label=idx2lab,\n",
    "    label2id=lab2idx\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d86b33",
   "metadata": {},
   "source": [
    "## 4. Tokenize IMDB Dataset Using Trained Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe6f7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_column_name = 'sents'\n",
    "label_column_name = 'ids'\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[text_column_name],\n",
    "        max_length=128,\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "        is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    all_labels = []\n",
    "    for batch_index, labels in enumerate(examples[label_column_name]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=batch_index)\n",
    "        label_ids = []\n",
    "        prev_word_id = None\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_id == prev_word_id:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(lab2idx[labels[word_id]])\n",
    "            prev_word_id = word_id\n",
    "        all_labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = all_labels\n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1f939a",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_imdb = imdb_data.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=imdb_data.column_names,\n",
    "    desc=\"Tokenizing IMDB reviews\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc54c34",
   "metadata": {},
   "source": [
    "## 5. Load Trained Model from Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a07fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../data/training_parameters/checkpoint-4704\"\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_path, config=config)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8865fe76",
   "metadata": {},
   "source": [
    "## 6. Run Predictions Using Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9260188f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_int_to_labels(preds):\n",
    "    logits, labels = preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    true_predictions = [\n",
    "        [idx2lab[pred] for pred, label in zip(pred_seq, label_seq) if label != -100]\n",
    "        for pred_seq, label_seq in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    return None, true_predictions  # You only need predicted labels here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2583caf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "training_args = TrainingArguments(output_dir=\"tmp\")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "predictions, labels, _ = trainer.predict(processed_imdb)\n",
    "_, predicted_labels = convert_int_to_labels((predictions, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6d2266",
   "metadata": {},
   "source": [
    "## 7. Save Predictions in CoNLL Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889f200f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_conll_file(data, path):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for sentence in data:\n",
    "            words, labels = sentence\n",
    "            for idx, (word, label) in enumerate(zip(words, labels), start=1):\n",
    "                f.write(f\"{idx}\\t{word}\\t{label}\\t-\\t-\\n\")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "\n",
    "imdb_tagged = [(tokens, labels) for tokens, labels in zip(df['tokens'].tolist(), predicted_labels)]\n",
    "#write_conll_file(imdb_tagged, \"../data/imdb_tagged_output.iob2\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3d410e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first tagged review as an example (safely aligned)\n",
    "tokens, labels = imdb_tagged[1]\n",
    "for idx in range(len(tokens)):\n",
    "    word = tokens[idx]\n",
    "    tag = labels[idx] if idx < len(labels) else \"O\"  # fallback if mismatch\n",
    "    print(f\"{idx+1}\\t{word}\\t{tag}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "websoup",
   "language": "python",
   "name": "websoup"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
