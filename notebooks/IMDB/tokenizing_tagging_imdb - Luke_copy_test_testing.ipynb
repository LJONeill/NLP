{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dcebe88",
   "metadata": {},
   "source": [
    "# Tagging Test IMDB Dataset using Baseline NER Model\n",
    "This notebook loads our trained NER model from a checkpoint and uses it to tag entities in the cleaned test split of the IMDB dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e79d2452",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../scripts\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2a8f7fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModelForTokenClassification, RobertaTokenizerFast, AutoConfig, Trainer, TrainingArguments, DataCollatorForTokenClassification\n",
    "from datasets import Dataset\n",
    "from span_f1 import readNlu, toSpans, getBegEnd, getLooseOverlap, getUnlabeled\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea25447",
   "metadata": {},
   "source": [
    "## 1. Load 10% test data IMDB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b3ea3434",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/test_9010.csv\")\n",
    "df['tokens'] = df['review'].apply(lambda x: x.split())\n",
    "df['dummy_labels'] = df['tokens'].apply(lambda x: ['O'] * len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937800f9",
   "metadata": {},
   "source": [
    "## 1b. Load 50% test data IMDB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "485054b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5050 = pd.read_csv(\"../data/test_5050.csv\")\n",
    "df5050['tokens'] = df5050['review'].apply(lambda x: x.split())\n",
    "df5050['dummy_labels'] = df5050['tokens'].apply(lambda x: ['O'] * len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504222bd",
   "metadata": {},
   "source": [
    "## 1c. Load 90% test data IMDB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5e0f6cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1090 = pd.read_csv(\"../data/test_1090.csv\")\n",
    "df1090['tokens'] = df1090['review'].apply(lambda x: x.split())\n",
    "df1090['dummy_labels'] = df1090['tokens'].apply(lambda x: ['O'] * len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe88f5fb",
   "metadata": {},
   "source": [
    "## 2. Create HuggingFace Dataset Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ac1d2038",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_9010_data = Dataset.from_dict({\n",
    "    'sents': df['tokens'].tolist(),\n",
    "    'ner_tags': df['dummy_labels'].tolist(),\n",
    "    'ids': df['dummy_labels'].tolist()\n",
    "})\n",
    "\n",
    "test_5050_data = Dataset.from_dict({\n",
    "    'sents': df5050['tokens'].tolist(),\n",
    "    'ner_tags': df5050['dummy_labels'].tolist(),\n",
    "    'ids': df5050['dummy_labels'].tolist()\n",
    "})\n",
    "\n",
    "test_1090_data = Dataset.from_dict({\n",
    "    'sents': df1090['tokens'].tolist(),\n",
    "    'ner_tags': df1090['dummy_labels'].tolist(),\n",
    "    'ids': df1090['dummy_labels'].tolist()\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d38b2fa",
   "metadata": {},
   "source": [
    "## 3. Load Tokenizer and Label Mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "20d6bf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load label mappings used during training\n",
    "with open('../project/baseline_model/idx2lab', 'rb') as f:\n",
    "    idx2lab = pickle.load(f)\n",
    "\n",
    "with open('../project/baseline_model/lab2idx', 'rb') as f:\n",
    "    lab2idx = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0ce26202",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label_list = list(lab2idx.keys())  # Needed for num_labels\n",
    "\n",
    "# Tokenizer and config\n",
    "model_link = \"deepset/roberta-base-squad2\"\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(model_link, use_fast=True, add_prefix_space=True)\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_link,\n",
    "    num_labels=len(label_list),\n",
    "    id2label=idx2lab,\n",
    "    label2id=lab2idx\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d86b33",
   "metadata": {},
   "source": [
    "## 4. Tokenize IMDB Dataset Using Trained Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1fe6f7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_column_name = 'sents'\n",
    "label_column_name = 'ids'\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[text_column_name],\n",
    "        max_length=128,\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "        is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    all_labels = []\n",
    "    for batch_index, labels in enumerate(examples[label_column_name]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=batch_index)\n",
    "        label_ids = []\n",
    "        prev_word_id = None\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_id == prev_word_id:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(lab2idx[labels[word_id]])\n",
    "            prev_word_id = word_id\n",
    "        all_labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = all_labels\n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8f1f939a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing 9010 reviews: 100%|██████████| 4959/4959 [00:03<00:00, 1479.99 examples/s]\n",
      "Tokenizing 5050 reviews: 100%|██████████| 24791/24791 [00:14<00:00, 1669.63 examples/s]\n",
      "Tokenizing 1090 reviews: 100%|██████████| 44623/44623 [00:28<00:00, 1555.21 examples/s]\n"
     ]
    }
   ],
   "source": [
    "processed_9010 = test_9010_data.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=imdb_data.column_names,\n",
    "    desc=\"Tokenizing 9010 reviews\"\n",
    ")\n",
    "\n",
    "processed_5050 = test_5050_data.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=imdb_data.column_names,\n",
    "    desc=\"Tokenizing 5050 reviews\"\n",
    ")\n",
    "\n",
    "processed_1090 = test_1090_data.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=imdb_data.column_names,\n",
    "    desc=\"Tokenizing 1090 reviews\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc54c34",
   "metadata": {},
   "source": [
    "## 5. Load Trained Model from Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66a07fd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForTokenClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"../project/baseline_model/output_trainer/checkpoint-4704\"\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_path, config=config)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8865fe76",
   "metadata": {},
   "source": [
    "## 6. Run Predictions Using Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9260188f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_int_to_labels(preds):\n",
    "    logits, labels = preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    true_predictions = [\n",
    "        [idx2lab[pred] for pred, label in zip(pred_seq, label_seq) if label != -100]\n",
    "        for pred_seq, label_seq in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    return None, true_predictions  # You only need predicted labels here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2583caf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "training_args = TrainingArguments(output_dir=\"tmp\")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "96ce5850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "predictions9010, labels9010, _ = trainer.predict(processed_9010)\n",
    "_9010, predicted_labels9010 = convert_int_to_labels((predictions9010, labels9010))\n",
    "\n",
    "predictions5050, labels5050, _ = trainer.predict(processed_5050)\n",
    "_5050, predicted_labels5050 = convert_int_to_labels((predictions5050, labels5050))\n",
    "\n",
    "predictions1090, labels1090, _ = trainer.predict(processed_1090)\n",
    "_1090, predicted_labels1090 = convert_int_to_labels((predictions1090, labels1090))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6d2266",
   "metadata": {},
   "source": [
    "## 7. Save Predictions in CoNLL Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889f200f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_conll_file(data, path):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for sentence in data:\n",
    "            words, labels = sentence\n",
    "            for idx, (word, label) in enumerate(zip(words, labels), start=1):\n",
    "                f.write(f\"{idx}\\t{word}\\t{label}\\t-\\t-\\n\")\n",
    "            f.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5b42218f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_9010 = [(tokens, labels) for tokens, labels in zip(df['tokens'].tolist(), predicted_labels9010)]\n",
    "write_conll_file(tagged_9010, \"../data/test_9010_tagged_output.iob2\")\n",
    "\n",
    "tagged_5050 = [(tokens, labels) for tokens, labels in zip(df5050['tokens'].tolist(), predicted_labels5050)]\n",
    "write_conll_file(tagged_5050, \"../data/test_5050_tagged_output.iob2\")\n",
    "\n",
    "tagged_1090 = [(tokens, labels) for tokens, labels in zip(df1090['tokens'].tolist(), predicted_labels1090)]\n",
    "write_conll_file(tagged_1090, \"../data/test_1090_tagged_output.iob2\")\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1a3d410e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\tNot\tO\n",
      "2\tgood\tO\n",
      "3\tMostly\tO\n",
      "4\tbecause\tO\n",
      "5\tyou\tO\n",
      "6\tdont\tO\n",
      "7\tgive\tO\n",
      "8\ta\tO\n",
      "9\tdamn\tO\n",
      "10\tabout\tO\n",
      "11\twhat\tO\n",
      "12\thappens\tO\n",
      "13\tto\tO\n",
      "14\tall\tO\n",
      "15\tthese\tO\n",
      "16\tpeople\tO\n",
      "17\tSome\tO\n",
      "18\tcomments\tO\n",
      "19\t1\tO\n",
      "20\tI\tO\n",
      "21\tam\tO\n",
      "22\ttired\tO\n",
      "23\tof\tO\n",
      "24\tseeing\tO\n",
      "25\tgovernesses\tO\n",
      "26\twho\tO\n",
      "27\tnever\tO\n",
      "28\ttalk\tO\n",
      "29\tto\tO\n",
      "30\ttheir\tO\n",
      "31\tpupils\tO\n",
      "32\tnever\tO\n",
      "33\tteach\tO\n",
      "34\tthem\tO\n",
      "35\tanything\tO\n",
      "36\tand\tO\n",
      "37\ttake\tO\n",
      "38\ta\tO\n",
      "39\ttired\tO\n",
      "40\tand\tO\n",
      "41\tannoyed\tO\n",
      "42\tlook\tO\n",
      "43\twhenever\tO\n",
      "44\tthe\tO\n",
      "45\tsaid\tO\n",
      "46\tpupil\tO\n",
      "47\twho\tO\n",
      "48\tof\tO\n",
      "49\tcourse\tO\n",
      "50\thas\tO\n",
      "51\tbeen\tO\n",
      "52\twon\tO\n",
      "53\tover\tO\n",
      "54\tin\tO\n",
      "55\tthe\tO\n",
      "56\tspace\tO\n",
      "57\tof\tO\n",
      "58\t4\tO\n",
      "59\tseconds\tO\n",
      "60\tsays\tO\n",
      "61\tsomething\tO\n",
      "62\t2\tO\n",
      "63\tFine\tO\n",
      "64\tso\tO\n",
      "65\tRosina\tB-PER\n",
      "66\thas\tO\n",
      "67\ta\tO\n",
      "68\tfather\tO\n",
      "69\tcomplex\tO\n",
      "70\tand\tO\n",
      "71\ttherefore\tO\n",
      "72\tis\tO\n",
      "73\tattracted\tO\n",
      "74\tto\tO\n",
      "75\ther\tO\n",
      "76\temployer\tO\n",
      "77\tBut\tO\n",
      "78\tCharles\tB-PER\n",
      "79\tis\tO\n",
      "80\tcompletely\tO\n",
      "81\tdifferent\tO\n",
      "82\tin\tO\n",
      "83\tall\tO\n",
      "84\taspects\tO\n",
      "85\tfrom\tO\n",
      "86\ther\tO\n",
      "87\tfather\tO\n",
      "88\tif\tO\n",
      "89\tanything\tO\n",
      "90\tHenry\tB-PER\n",
      "91\tis\tO\n",
      "92\tmuch\tO\n",
      "93\tcloser\tO\n",
      "94\tas\tO\n",
      "95\ta\tO\n",
      "96\tsensual\tO\n",
      "97\texalted\tO\n",
      "98\tperson\tO\n",
      "99\t3\tO\n",
      "100\tHow\tO\n",
      "101\tcould\tO\n",
      "102\tyou\tO\n",
      "103\tever\tO\n",
      "104\tbelieve\tO\n",
      "105\tthat\tO\n",
      "106\tshe\tO\n",
      "107\twould\tO\n",
      "108\tbe\tO\n",
      "109\tmore\tO\n",
      "110\tattracted\tO\n",
      "111\tto\tO\n",
      "112\tTom\tB-PER\n",
      "113\tWilkinson\tI-PER\n",
      "114\tthan\tO\n",
      "115\tto\tO\n",
      "116\tRhys\tB-PER\n",
      "117\tMeyer\tI-PER\n",
      "118\t4\tO\n",
      "119\tHard\tO\n",
      "120\tto\tO\n",
      "121\tbelieve\tO\n",
      "122\tif\tO\n",
      "123\tshe\tO\n",
      "124\thad\tO\n",
      "125\tbeen\tO\n",
      "126\tin\tO\n",
      "127\tfact\tO\n",
      "128\traised\tO\n",
      "129\tas\tO\n",
      "130\ta\tO\n",
      "131\tdeeply\tO\n",
      "132\treligious\tO\n",
      "133\tgirl\tO\n",
      "134\tthat\tO\n",
      "135\tshe\tO\n",
      "136\twould\tO\n",
      "137\tbe\tO\n",
      "138\tso\tO\n",
      "139\tcareless\tO\n",
      "140\tabout\tO\n",
      "141\tsleeping\tO\n",
      "142\twith\tO\n",
      "143\ta\tO\n",
      "144\tgentile\tO\n",
      "145\tafter\tO\n",
      "146\tknowing\tO\n",
      "147\thim\tO\n",
      "148\tfor\tO\n",
      "149\t5\tO\n",
      "150\tminutesbr\tO\n",
      "151\tSome\tO\n",
      "152\tgood\tO\n",
      "153\tthings\tO\n",
      "154\tabout\tO\n",
      "155\tthe\tO\n",
      "156\tfilm\tO\n",
      "157\tAt\tO\n",
      "158\tleast\tO\n",
      "159\tshe\tO\n",
      "160\tdidnt\tO\n",
      "161\tend\tO\n",
      "162\tup\tO\n",
      "163\tpregnant\tO\n",
      "164\tnot\tO\n",
      "165\tknowing\tO\n",
      "166\twho\tO\n",
      "167\tthe\tO\n",
      "168\tfather\tO\n",
      "169\twas\tO\n",
      "170\tThe\tO\n",
      "171\twhole\tO\n",
      "172\tdescription\tO\n",
      "173\tof\tO\n",
      "174\tlife\tO\n",
      "175\tin\tO\n",
      "176\tthe\tO\n",
      "177\tJewish\tO\n",
      "178\tcommunity\tO\n",
      "179\tin\tO\n",
      "180\tLondon\tO\n",
      "181\tis\tO\n",
      "182\tgood\tO\n"
     ]
    }
   ],
   "source": [
    "# Print the first tagged review as an example (safely aligned)\n",
    "tokens, labels = imdb_tagged[1]\n",
    "for idx in range(len(tokens)):\n",
    "    word = tokens[idx]\n",
    "    tag = labels[idx] if idx < len(labels) else \"O\"  # fallback if mismatch\n",
    "    print(f\"{idx+1}\\t{word}\\t{tag}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
