{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "087f72b8",
   "metadata": {},
   "source": [
    "## References:\n",
    "\n",
    "The json catalogs have been retrieved from the following locations:\n",
    "\n",
    "\"fortune500.json\": https://github.com/dariusk/corpora/blob/master/data/corporations/fortune500.json\n",
    "orgs\n",
    "\n",
    "\"basic.json\" and \"names.json\": https://github.com/marcotcr/checklist/tree/master/checklist/data\n",
    "loc per\n",
    "\n",
    "Most code has been made using generative AI (chatGPT 4o)\n",
    "\n",
    "Pertubation will only be done on the 90/10 split model, as this was determined to have the highest accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2328c227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00c4e24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preps n sentences (so we can choose the size of our invariance test set) for entity swapping\n",
    "def prep_n_sentences(filepath, n):\n",
    "    sentences = [] #empty list with all sentences\n",
    "    sentence = [] #the current sentence\n",
    "\n",
    "\n",
    "    #generating tuples of words and their tags and appending to sentence\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            stripped = line.strip()\n",
    "            if not stripped:\n",
    "                if sentence:\n",
    "                    sentences.append(sentence)\n",
    "                    sentence = []\n",
    "                    if len(sentences) >=n:\n",
    "                        break\n",
    "                continue\n",
    "            parts = stripped.split()\n",
    "            if len(parts)>=3:\n",
    "                token = parts[1]\n",
    "                tag = parts[2]\n",
    "                sentence.append((token,tag))\n",
    "    \n",
    "    #if not empty, append to sentences list\n",
    "    if sentence:\n",
    "        sentences.append(sentence)\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "869d3673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_named_entities(sentence, name_dict, location_dict, organisations_dict):\n",
    "    swapped_sentence = []\n",
    "    i = 0\n",
    "    while i < len(sentence):\n",
    "        token, tag = sentence[i]\n",
    "\n",
    "        if tag.startswith(\"B-\"):\n",
    "            entity_type = tag[2:]  # e.g., 'PER', 'LOC', 'ORG'\n",
    "            entity_tokens = [token]\n",
    "            j = i + 1\n",
    "\n",
    "            # Collect the rest of the entity\n",
    "            while j < len(sentence) and sentence[j][1] == f\"I-{entity_type}\":\n",
    "                entity_tokens.append(sentence[j][0])\n",
    "                j += 1\n",
    "\n",
    "            # Swap the entity\n",
    "            if entity_type == \"PER\":\n",
    "                firstname_sex = random.choice([\"men\",\"women\"])\n",
    "                if len(entity_tokens) == 1:\n",
    "                    new_entity = [random.choice(name_dict[firstname_sex])]\n",
    "                else:\n",
    "                    new_entity = [random.choice(name_dict[firstname_sex])]\n",
    "                    new_entity += [random.choice(name_dict[\"last\"]) for _ in range(len(entity_tokens) - 1)]\n",
    "            elif entity_type == \"ORG\":\n",
    "                new_entity = random.choice(organisations_dict[\"companies\"]).split()\n",
    "            elif entity_type == \"LOC\":\n",
    "                loc_choice = random.choice([\"city\", \"country\"])\n",
    "                new_entity = random.choice(location_dict[loc_choice]).split()\n",
    "            else:\n",
    "                new_entity = entity_tokens  # fallback: no swap\n",
    "\n",
    "            # Apply new entity with correct tags\n",
    "            swapped_sentence.append((new_entity[0], f\"B-{entity_type}\"))\n",
    "            for tok in new_entity[1:]:\n",
    "                swapped_sentence.append((tok, f\"I-{entity_type}\"))\n",
    "\n",
    "            i = j  # move past the original entity\n",
    "        else:\n",
    "            swapped_sentence.append((token, tag))\n",
    "            i += 1\n",
    "\n",
    "    return swapped_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89ef067c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dir_sentiment(sentence, label, sentiment_dict):\n",
    "    if label == 1:  # Positive\n",
    "        addon = random.choice(sentiment_dict[\"positive\"])\n",
    "    else:  # Negative\n",
    "        addon = random.choice(sentiment_dict[\"negative\"])\n",
    "    return f\"{sentence} {addon}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04c914e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_filepath = \"../data/imdb_test_train_datasets/test_tagged/test_9010_tagged_output.iob2\"\n",
    "\n",
    "names_filepath = \"../data/perturb_json_files/names.json\"\n",
    "with open(names_filepath, \"r\") as js:\n",
    "    names_dict = json.load(js)\n",
    "\n",
    "locations_filepath = \"../data/perturb_json_files/basic.json\"\n",
    "with open(locations_filepath, \"r\") as js:\n",
    "    locations_dict = json.load(js)\n",
    "\n",
    "org_filepath = \"../data/perturb_json_files/fortune500.json\"\n",
    "with open(org_filepath, \"r\") as js:\n",
    "    orgs_dict = json.load(js)\n",
    "\n",
    "sentiment_filepath = \"../data/perturb_json_files/sentiment_dict.json\"\n",
    "with open(sentiment_filepath, \"r\") as js:\n",
    "    sentiment_dict = json.load(js)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0720047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the original labels from test_9010.csv\n",
    "label_df = pd.read_csv(\"../data/imdb_test_train_datasets/test/test_9010.csv\")\n",
    "reviews = label_df[\"review\"].tolist()\n",
    "labels = label_df[\"label\"].tolist()\n",
    "\n",
    "n = len(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1354b380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Written 4959 swapped sentences to ../data/imdb_test_train_datasets/perturbed/invariance_swapped_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Parse a subset or the full file\n",
    "sentences = prep_n_sentences(tagged_filepath, n)\n",
    "\n",
    "# Swap entities\n",
    "swapped_sentences = [swap_named_entities(sent, names_dict, locations_dict, orgs_dict) for sent in sentences]\n",
    "\n",
    "# Pair each swapped sentence with its corresponding label\n",
    "output_path = \"../data/imdb_test_train_datasets/perturbed/invariance_swapped_data.csv\"\n",
    "perturbed_with_labels = [\n",
    "    (\" \".join(token for token, _ in sent), labels[i])\n",
    "    for i, sent in enumerate(swapped_sentences)\n",
    "]\n",
    "\n",
    "# Write to CSV\n",
    "with open(output_path, mode='w', encoding='utf-8', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"review\", \"label\"])  # header\n",
    "    for review, label in perturbed_with_labels:\n",
    "        writer.writerow([review, label])\n",
    "\n",
    "print(f\"✅ Written {len(perturbed_with_labels)} swapped sentences to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa74db93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Written 4959 sentiment-augmented reviews to ../data/imdb_test_train_datasets/perturbed/directional_expectation_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Directional expectation\n",
    "augmented_data = [\n",
    "    (add_dir_sentiment(review, label, sentiment_dict), label)\n",
    "    for review, label in zip(reviews, labels)\n",
    "]\n",
    "\n",
    "# Write to CSV\n",
    "output_path = \"../data/imdb_test_train_datasets/perturbed/directional_expectation_data.csv\"\n",
    "with open(output_path, mode='w', encoding='utf-8', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"review\", \"label\"])  # header\n",
    "    for review, label in augmented_data:\n",
    "        writer.writerow([review, label])\n",
    "\n",
    "print(f\"✅ Written {len(augmented_data)} sentiment-augmented reviews to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
