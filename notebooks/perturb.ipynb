{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "087f72b8",
   "metadata": {},
   "source": [
    "## References:\n",
    "\n",
    "The json catalogs have been retrieved from the following locations:\n",
    "\n",
    "\"fortune500.json\": https://github.com/dariusk/corpora/blob/master/data/corporations/fortune500.json\n",
    "\n",
    "\"basic.json\" and \"names.json\": https://github.com/marcotcr/checklist/tree/master/checklist/data\n",
    "\n",
    "Most code has been made using generative AI (chatGPT 4o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2328c227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c4e24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preps n sentences (so we can choose the size of our invariance test set) for entity swapping\n",
    "def prep_n_sentences(filepath, n):\n",
    "    sentences = [] #empty list with all sentences\n",
    "    sentence = [] #the current sentence\n",
    "\n",
    "\n",
    "    #generating tuples of words and their tags and appending to sentence\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            stripped = line.strip()\n",
    "            if not stripped:\n",
    "                if sentence:\n",
    "                    sentences.append(sentence)\n",
    "                    sentence = []\n",
    "                    if len(sentences) >=n:\n",
    "                        break\n",
    "                continue\n",
    "            parts = stripped.split()\n",
    "            if len(parts)>=3:\n",
    "                token = parts[1]\n",
    "                tag = parts[2]\n",
    "                sentence.append((token,tag))\n",
    "    \n",
    "    #if not empty, append to sentences list\n",
    "    if sentence:\n",
    "        sentences.append(sentence)\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869d3673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_named_entities(sentence, name_dict, location_dict, organisations_dict):\n",
    "    swapped_sentence = []\n",
    "    i = 0\n",
    "    while i < len(sentence):\n",
    "        token, tag = sentence[i]\n",
    "\n",
    "        if tag.startswith(\"B-\"):\n",
    "            entity_type = tag[2:]  # e.g., 'PER', 'LOC', 'ORG'\n",
    "            entity_tokens = [token]\n",
    "            j = i + 1\n",
    "\n",
    "            # Collect the rest of the entity\n",
    "            while j < len(sentence) and sentence[j][1] == f\"I-{entity_type}\":\n",
    "                entity_tokens.append(sentence[j][0])\n",
    "                j += 1\n",
    "\n",
    "            # Swap the entity\n",
    "            if entity_type == \"PER\":\n",
    "                firstname_sex = random.choice([\"men\",\"women\"])\n",
    "                if len(entity_tokens) == 1:\n",
    "                    new_entity = [random.choice(name_dict[firstname_sex])]\n",
    "                else:\n",
    "                    new_entity = [random.choice(name_dict[firstname_sex])]\n",
    "                    new_entity += [random.choice(name_dict[\"last\"]) for _ in range(len(entity_tokens) - 1)]\n",
    "            elif entity_type == \"ORG\":\n",
    "                new_entity = random.choice(organisations_dict[\"companies\"]).split()\n",
    "            elif entity_type == \"LOC\":\n",
    "                loc_choice = random.choice([\"city\", \"country\"])\n",
    "                new_entity = random.choice(location_dict[loc_choice]).split()\n",
    "            else:\n",
    "                new_entity = entity_tokens  # fallback: no swap\n",
    "\n",
    "            # Apply new entity with correct tags\n",
    "            swapped_sentence.append((new_entity[0], f\"B-{entity_type}\"))\n",
    "            for tok in new_entity[1:]:\n",
    "                swapped_sentence.append((tok, f\"I-{entity_type}\"))\n",
    "\n",
    "            i = j  # move past the original entity\n",
    "        else:\n",
    "            swapped_sentence.append((token, tag))\n",
    "            i += 1\n",
    "\n",
    "    return swapped_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ef067c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sentiment(sentence, sentiment_dict, sentiment = \"pos\"):\n",
    "    tokens = [token for token, _ in sentence] #getting all tokens from sentence\n",
    "\n",
    "    joined_sentence = \" \".join(tokens) #recreating original sentence\n",
    "    \n",
    "    if sentiment == \"pos\": #if selected sentiment is positive\n",
    "        addon = random.choice(sentiment_dict[\"positive\"]) #selects a random add-on sentence from the positive list in the json\n",
    "    \n",
    "    else:\n",
    "        addon = random.choice(sentiment_dict[\"negative\"]) #same as above but negative\n",
    "    \n",
    "    final_sentence = f\"{joined_sentence} {addon}\"\n",
    "\n",
    "    return final_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c914e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_filepath = \"../data/imdb_tagged_output.iob2\"\n",
    "\n",
    "names_filepath = \"../data/names.json\"\n",
    "with open(names_filepath, \"r\") as js:\n",
    "    names_dict = json.load(js)\n",
    "\n",
    "locations_filepath = \"../data/basic.json\"\n",
    "with open(locations_filepath, \"r\") as js:\n",
    "    locations_dict = json.load(js)\n",
    "\n",
    "org_filepath = \"../data/fortune500.json\"\n",
    "with open(org_filepath, \"r\") as js:\n",
    "    orgs_dict = json.load(js)\n",
    "\n",
    "sentiment_filepath = \"../data/sentiment_dict.json\"\n",
    "with open(sentiment_filepath, \"r\") as js:\n",
    "    sentiment_dict = json.load(js)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1d689a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse a subset or the full file\n",
    "sentences = prep_n_sentences(tagged_filepath, 1000)\n",
    "\n",
    "# Swap entities\n",
    "swapped_sentences = [swap_named_entities(sent, names_dict, locations_dict, orgs_dict) for sent in sentences]\n",
    "\n",
    "# Convert to plain strings\n",
    "sentence_strings = [\" \".join(token for token, tag in sent) for sent in swapped_sentences]\n",
    "\n",
    "# Write one sentence per line in a new file\n",
    "with open(\"../data/invariance_swapped_data.txt\", 'w', encoding='utf-8') as f:\n",
    "    for line in sentence_strings:\n",
    "        f.write(line + '\\n')\n",
    "\n",
    "print(f\"✅ Written {len(sentence_strings)} swapped sentences to invariance_swapped_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa74db93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this cell assumes you've run the cell above, otherwise please run the commented-out cell below:\n",
    "#sentences = prep_n_sentences(tagged_filepath, 1000)\n",
    "\n",
    "#add sentiment-sentences, default is positive\n",
    "augmented_sentences = [add_sentiment(sent, sentiment_dict) for sent in sentences]\n",
    "\n",
    "#Write one sentence per line in a new file\n",
    "with open(\"../data/dir_augmented_data.txt\", 'w', encoding='utf-8') as f:\n",
    "    for line in augmented_sentences:\n",
    "        f.write(line + '\\n')\n",
    "\n",
    "print(f\"✅ Written {len(augmented_sentences)} swapped sentences to dir_augmented_data.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
