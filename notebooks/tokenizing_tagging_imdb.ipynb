{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dcebe88",
   "metadata": {},
   "source": [
    "# Tagging Cleaned IMDB Dataset using Baseline NER Model\n",
    "This notebook loads your trained NER model from a checkpoint and uses it to tag entities in the cleaned IMDB dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e79d2452",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../scripts\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a8f7fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModelForTokenClassification, RobertaTokenizerFast, AutoConfig, Trainer, TrainingArguments, DataCollatorForTokenClassification\n",
    "from datasets import Dataset\n",
    "from span_f1 import readNlu, toSpans, getBegEnd, getLooseOverlap, getUnlabeled\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea25447",
   "metadata": {},
   "source": [
    "## 1. Load Cleaned IMDB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3ea3434",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/clean_imdb_dataset.csv\")\n",
    "df['tokens'] = df['review'].apply(lambda x: x.split())\n",
    "df['dummy_labels'] = df['tokens'].apply(lambda x: ['O'] * len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe88f5fb",
   "metadata": {},
   "source": [
    "## 2. Create HuggingFace Dataset Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac1d2038",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_data = Dataset.from_dict({\n",
    "    'sents': df['tokens'].tolist(),\n",
    "    'ner_tags': df['dummy_labels'].tolist(),\n",
    "    'ids': df['dummy_labels'].tolist()\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d38b2fa",
   "metadata": {},
   "source": [
    "## 3. Load Tokenizer and Label Mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20d6bf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load label mappings used during training\n",
    "with open('../project/baseline_model/idx2lab', 'rb') as f:\n",
    "    idx2lab = pickle.load(f)\n",
    "\n",
    "with open('../project/baseline_model/lab2idx', 'rb') as f:\n",
    "    lab2idx = pickle.load(f)\n",
    "\n",
    "label_list = list(lab2idx.keys())  # Needed for num_labels\n",
    "\n",
    "# Tokenizer and config\n",
    "model_link = \"deepset/roberta-base-squad2\"\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(model_link, use_fast=True, add_prefix_space=True)\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_link,\n",
    "    num_labels=len(label_list),\n",
    "    id2label=idx2lab,\n",
    "    label2id=lab2idx\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d86b33",
   "metadata": {},
   "source": [
    "## 4. Tokenize IMDB Dataset Using Trained Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1fe6f7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_column_name = 'sents'\n",
    "label_column_name = 'ids'\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[text_column_name],\n",
    "        max_length=128,\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "        is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    all_labels = []\n",
    "    for batch_index, labels in enumerate(examples[label_column_name]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=batch_index)\n",
    "        label_ids = []\n",
    "        prev_word_id = None\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_id == prev_word_id:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(lab2idx[labels[word_id]])\n",
    "            prev_word_id = word_id\n",
    "        all_labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = all_labels\n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f1f939a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbfef5bbbe4749a49c542863b61c770a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing IMDB reviews:   0%|          | 0/49581 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processed_imdb = imdb_data.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=imdb_data.column_names,\n",
    "    desc=\"Tokenizing IMDB reviews\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc54c34",
   "metadata": {},
   "source": [
    "## 5. Load Trained Model from Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66a07fd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForTokenClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"../data/training_parameters/checkpoint-4704\"\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_path, config=config)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8865fe76",
   "metadata": {},
   "source": [
    "## 6. Run Predictions Using Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9260188f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_int_to_labels(preds):\n",
    "    logits, labels = preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    true_predictions = [\n",
    "        [idx2lab[pred] for pred, label in zip(pred_seq, label_seq) if label != -100]\n",
    "        for pred_seq, label_seq in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    return None, true_predictions  # You only need predicted labels here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2583caf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "training_args = TrainingArguments(output_dir=\"tmp\")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "predictions, labels, _ = trainer.predict(processed_imdb)\n",
    "_, predicted_labels = convert_int_to_labels((predictions, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6d2266",
   "metadata": {},
   "source": [
    "## 7. Save Predictions in CoNLL Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "889f200f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_conll_file(data, path):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for sentence in data:\n",
    "            words, labels = sentence\n",
    "            for idx, (word, label) in enumerate(zip(words, labels), start=1):\n",
    "                f.write(f\"{idx}\\t{word}\\t{label}\\t-\\t-\\n\")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "\n",
    "imdb_tagged = [(tokens, labels) for tokens, labels in zip(df['tokens'].tolist(), predicted_labels)]\n",
    "write_conll_file(imdb_tagged, \"../data/imdb_tagged_output.iob2\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a3d410e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\tA\tO\n",
      "2\twonderful\tO\n",
      "3\tlittle\tO\n",
      "4\tproduction\tO\n",
      "5\tThe\tO\n",
      "6\tfilming\tO\n",
      "7\ttechnique\tO\n",
      "8\tis\tO\n",
      "9\tvery\tO\n",
      "10\tunassuming\tO\n",
      "11\tvery\tO\n",
      "12\toldtimeBBC\tO\n",
      "13\tfashion\tO\n",
      "14\tand\tO\n",
      "15\tgives\tO\n",
      "16\ta\tO\n",
      "17\tcomforting\tO\n",
      "18\tand\tO\n",
      "19\tsometimes\tO\n",
      "20\tdiscomforting\tO\n",
      "21\tsense\tO\n",
      "22\tof\tO\n",
      "23\trealism\tO\n",
      "24\tto\tO\n",
      "25\tthe\tO\n",
      "26\tentire\tO\n",
      "27\tpiece\tO\n",
      "28\tThe\tO\n",
      "29\tactors\tO\n",
      "30\tare\tO\n",
      "31\textremely\tO\n",
      "32\twell\tO\n",
      "33\tchosen\tO\n",
      "34\tMichael\tB-PER\n",
      "35\tSheen\tI-PER\n",
      "36\tnot\tO\n",
      "37\tonly\tO\n",
      "38\thas\tO\n",
      "39\tgot\tO\n",
      "40\tall\tO\n",
      "41\tthe\tO\n",
      "42\tpolari\tO\n",
      "43\tbut\tO\n",
      "44\the\tO\n",
      "45\thas\tO\n",
      "46\tall\tO\n",
      "47\tthe\tO\n",
      "48\tvoices\tO\n",
      "49\tdown\tO\n",
      "50\tpat\tO\n",
      "51\ttoo\tO\n",
      "52\tYou\tO\n",
      "53\tcan\tO\n",
      "54\ttruly\tO\n",
      "55\tsee\tO\n",
      "56\tthe\tO\n",
      "57\tseamless\tO\n",
      "58\tediting\tO\n",
      "59\tguided\tO\n",
      "60\tby\tO\n",
      "61\tthe\tO\n",
      "62\treferences\tO\n",
      "63\tto\tO\n",
      "64\tWilliams\tB-PER\n",
      "65\tdiary\tO\n",
      "66\tentries\tO\n",
      "67\tnot\tO\n",
      "68\tonly\tO\n",
      "69\tis\tO\n",
      "70\tit\tO\n",
      "71\twell\tO\n",
      "72\tworth\tO\n",
      "73\tthe\tO\n",
      "74\twatching\tO\n",
      "75\tbut\tO\n",
      "76\tit\tO\n",
      "77\tis\tO\n",
      "78\ta\tO\n",
      "79\tterrificly\tO\n",
      "80\twritten\tO\n",
      "81\tand\tO\n",
      "82\tperformed\tO\n",
      "83\tpiece\tO\n",
      "84\tA\tO\n",
      "85\tmasterful\tO\n",
      "86\tproduction\tO\n",
      "87\tabout\tO\n",
      "88\tone\tO\n",
      "89\tof\tO\n",
      "90\tthe\tO\n",
      "91\tgreat\tO\n",
      "92\tmasters\tO\n",
      "93\tof\tO\n",
      "94\tcomedy\tO\n",
      "95\tand\tO\n",
      "96\this\tO\n",
      "97\tlife\tO\n",
      "98\tThe\tO\n",
      "99\trealism\tO\n",
      "100\treally\tO\n",
      "101\tcomes\tO\n",
      "102\thome\tO\n",
      "103\twith\tO\n",
      "104\tthe\tO\n",
      "105\tlittle\tO\n",
      "106\tthings\tO\n",
      "107\tthe\tO\n",
      "108\tfantasy\tO\n",
      "109\tof\tO\n",
      "110\tthe\tO\n",
      "111\tguard\tO\n",
      "112\twhich\tO\n",
      "113\trather\tO\n",
      "114\tthan\tO\n",
      "115\tuse\tO\n",
      "116\tthe\tO\n",
      "117\ttraditional\tO\n",
      "118\tdream\tO\n",
      "119\ttechniques\tO\n",
      "120\tremains\tO\n",
      "121\tsolid\tO\n",
      "122\tthen\tO\n",
      "123\tdisappears\tO\n",
      "124\tIt\tO\n",
      "125\tplays\tO\n",
      "126\ton\tO\n",
      "127\tour\tO\n",
      "128\tknowledge\tO\n",
      "129\tand\tO\n",
      "130\tour\tO\n",
      "131\tsenses\tO\n",
      "132\tparticularly\tO\n",
      "133\twith\tO\n",
      "134\tthe\tO\n",
      "135\tscenes\tO\n",
      "136\tconcerning\tO\n",
      "137\tOrton\tO\n",
      "138\tand\tO\n",
      "139\tHalliwell\tO\n",
      "140\tand\tO\n",
      "141\tthe\tO\n",
      "142\tsets\tO\n",
      "143\tparticularly\tO\n",
      "144\tof\tO\n",
      "145\ttheir\tO\n",
      "146\tflat\tO\n",
      "147\twith\tO\n",
      "148\tHalliwells\tO\n",
      "149\tmurals\tO\n",
      "150\tdecorating\tO\n",
      "151\tevery\tO\n",
      "152\tsurface\tO\n",
      "153\tare\tO\n",
      "154\tterribly\tO\n",
      "155\twell\tO\n",
      "156\tdone\tO\n"
     ]
    }
   ],
   "source": [
    "# Print the first tagged review as an example (safely aligned)\n",
    "tokens, labels = imdb_tagged[1]\n",
    "for idx in range(len(tokens)):\n",
    "    word = tokens[idx]\n",
    "    tag = labels[idx] if idx < len(labels) else \"O\"  # fallback if mismatch\n",
    "    print(f\"{idx+1}\\t{word}\\t{tag}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "websoup",
   "language": "python",
   "name": "websoup"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
